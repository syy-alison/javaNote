# 1. 概述

- **解决了哪些问题？**

 利用高可用服务、智能迁移服务、数据同步服务以及自动化运维平台构建了一套完整的分布式缓存系统，解决了业务应用直接使用开源缓存系统在大规模服务场景下遇到的可用性、性能以及扩展性问题。

- **适用场景**

对延迟敏感并且缓存数据量小（500G以内）的业务研发工程师建议使用redis，主要场景有：

一，对延迟敏感的基础公共服务，比如账号服务。

二，突发的系统访问量、流量，常见的比如热点视频。

三，大型促销秒杀系统或者带有计数系统的库存系统。

四，特殊的数据结构需求，简化开发模型，如排行榜、过滤器等可以直接使用redis提供的SortedSet以及Bit来简化实现。

- **不适用场景**

一、数据量非常大的，比如超过了500G。

二，单个缓存超过了1M。

三，对数据可靠性要求极高，任何情况下不能丢数据的应用场景不适合使用Squirrel

- **通过以下手段保证数据的可靠性：**

一，多副本存储策略：同一份数据的多副本存储，保证一个副本宕机情况下其他的副本依旧有全量的数据。

二，多机房部署容灾策略：将多个副本部署在不同的机房中，避免机房掉电及断网等极端场景下带来的数据丢失。

三，持久化策略：将数据持久化到硬盘中，可保证所有副本宕机情况下数据不丢失。

- **机房容灾**

支持集群的三机房部署，集群的每组节点平均分布在三个机房，三机房部署的集群架构可以满足任意一个机房不可用的情况下，其他机房的节点能够正常的提供服务。

- **主从节点的一致性**

为了追求高性能，主从节点通过单向数据流来保证数据的弱一致性，即主从副本之间的数据存在不一致时间窗口，窗口时间主要取决于主从之间的网络以及节点的负载情况，正常情况下是毫秒级别。

- **如何扩容**

整个扩容过程分为扩容新节点和数据迁移两个阶段。通过弹性伸缩系统可完成对新节点的秒级扩容。新节点加入到集群后需要通过数据迁移服务对集群内的数据重分配，整个数据迁移过程所消耗的时间需要参考实际的集群内节点分布情况，一般需要5~30分钟。

## 1.1. 架构

![img](https://cdn.nlark.com/yuque/0/2024/jpeg/42819892/1716281448189-4830970c-49ec-4491-9564-63896fe166d8.jpeg)

| 模块             | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| Redis-Client-SDK | Redis客户端，提供用户直连Redis集群作用。目前提供 Java、C/C++、NodeJs、Go、Python语言客户端 |
| Redis-WEB        | Redis的运维管理平台，负责整个Redis 的运维、运营、用户操作等功能。 |
| Redis-HA         | Redis的高可用保障服务，在集群发生故障时进行相应的集群状态恢复操作 |
| Redis-Migrate    | Redis 数据服务，提供集群内的数据迁移，数据清理等功能         |
| Redis-Monitor    | Redis 监控告警服务                                           |
| Redis-Schedule   | Redis 调度，负责对资源的合理调度                             |
| Redis-Keeper     | 集群组内集群之间的实时同步系统                               |

# 2. 基础知识

## 2.1. 数据结构

相比于memcache 作为缓存服务，redis 提供了更为丰富的数据结构：String， List，Set，SortedSet，Hash等。对于这五种数据结构，可以结合Java中的对应的类来进行理解，其中String数据结构对应Object类 （任意对象都会序列化成string来存储），List数据结构对应java.util.List接口的实现类java.util.LinkedList，Set数据结构对应java.util.Set接口，SortedSet数据结构对应java.util.SortedSet接口，Hash数据结构对应java.util.HashMap类。下面简单介绍下几种数据结构以及使用场景。

### 2.1.1. String

String类型是Redis中最为基础的数据存储类型，是二进制安全的字符串，该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中String类型的Value最多可以容纳的数据长度是512M，在squirrel-client中Value限制的大小是1M。相对于其他的几种数据结构，只有String类型的命令在写入key的时候可以带有默认的过期时间（在squirrel-client中，对于String类型的命令只有set,add和multiset命令会自动设置过期时间，且过期时间为使用category的过期时间），对于其他的数据结构，key默认是不过期的，如果需要设置过期时间，必须显示调用expire函数设置过期时间。**在redis-client中，所有的对象都会被序列化成String存到集群中，因此所有的数据都可以作为String类型来存储**。

使用场景：value较小、模型简单的 value可以使用String类型存储，对于一些特殊的数据结构，比如List、Set等，建议采用相应的下面介绍的List和Set数据结构进行存储，这样不仅可以节省存储空间还可以提高操作效率。

### 2.1.2. List

List类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表一样，可以在其头部(left)和尾部(right)添加新的元素。在插入时，如果该键并不存在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除，那么该键也将会被从数据库中删除。

从元素插入和删除的效率视角来看，如果是在链表的两头插入或删除元素，这将会是非常高效的操作，即使链表中已经存储了百万条记录，该操作也可以在常量时间内完成。然而需要说明的是，如果元素插入或删除操作是作用于链表中间，那将会是非常低效的。

使用场景：在评级系统中，比如社会化新闻网站，你可以把每个新提交的链接添加到一个list，用LRANGE可简单的对结果分页；在博客引擎实现中，你可为每篇日志设置一个list，在该list中推入进博客评论等等。

### 2.1.3. Set

Set类型是没有排序的字符串集合，和List类型一样，也可以在该类型的数据值上执行添加、删除或判断某一元素是否存在等操作。和List类型不同的是，Set集合中不允许出现重复的元素，如果多次添加相同元素，Set中将仅保留该元素的一份拷贝。

使用场景：可以使用Redis的Set数据类型跟踪一些唯一性数据，比如访问某一博客的唯一IP地址信息。对于此场景，仅需在每次访问该博客时将访问者的IP存入Redis中，Set数据类型会自动保证IP地址的唯一性。

### 2.1.4. SortedSet

SortedSet和Set类型极为相似，它们都是字符串的集合，都不允许重复的成员出现在一个Set中。它们之间的主要差别是SortedSet中的每一个成员都会有一个分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。需要额外指出的是，尽管SortedSet中的成员必须是唯一的，但是分数(score)却是可以重复的。在SortedSet中添加、删除或更新一个成员都是非常快速的操作，其时间复杂度为O(logn)。由于SortedSet中的成员在集合中的位置是有序的，因此，即便是访问位于集合中部的成员也仍然是非常高效的。

使用场景：

1. 可以用于一个大型在线游戏的积分排行榜。每当玩家的分数发生变化时，可以执行ZADD命令更新玩家的分数，此后再通过ZRANGE命令获取积分TOP TEN的用户信息。当然也可以利用ZRANK命令通过username来获取玩家的排行信息。最后将组合使用ZRANGE和ZRANK命令快速的获取和某个玩家积分相近的其他用户的信息。
2. SortedSet类型还可用于构建索引数据。
3. 建立一个SortedSet中元素个数不要超过 1 W。

### 2.1.5. Hash

Hash类型相当于Java中的HashMap。所以该类型非常适合于存储值对象的信息，比如User对象含有Username、Password和Age等属性，可以使用hash来存储User，每个field对应一个属性，好处是可以做到部分更新、获取。如果Hash中包含很少的字段，那么该类型的数据也将仅占用很少的磁盘空间。

使用场景：

1，对于海量数据的情况，可以自己对数据进行分桶，然后使用Hash结构来存储。对于很多value为简单的字符串，做过测试，采用hash存储更节省空间。

2，将对象存储为Hash结构而不是String，可以每次只更新、获取Hash中的一个field，这样可以提高效率。

### 2.1.6. HyperLogLog

HyperLogLog类型用来进行基数统计。利用HyperLogLog，用户可以使用少量固定大小的内存，来统计集合中唯一元素的数量（每个HyperLogLog占用12KB内存，可以计算接近264个不同元素的基数）。关于HyperLogLog的原理，可以参见：[神奇的HyperLogLog算法](https://blog.csdn.net/firenet1/article/details/77247649)

利用HyperLogLog得到的基数统计结果，不是精确值，而是一个带有0.81%标准差（standard error）的近似值。所以，HyperLogLog适用于一些对于统计结果精确度要求不是特别高的场景。

使用场景：

1.可以用于统计一个网站的UV。利用HyperLogLog来统计访问一个网站的不同ip的个数。

## 2.2. 持久化

### 2.2.1. **RDB & AOF 简介**

Redis 提供了两种持久化策略

- RDB 持久化机制，会在一段时间内生成指定时间点的数据集**快照(*****snapshot\*****)**
- AOF 持久化机制，记录 server 端收到的每一条写命令，当 server 重启时会进行重放以此来重建之前的数据集。AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被**追加(*****append\*****)**到文件的末尾。 Redis 还可以在后台对 AOF 文件进行**重写(*****rewrite\*****)** ，使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。
- 如果你仅使用 Redis 作为缓存加速访问，你可以关闭这两个持久化设置
- 你也可以同时开启这两个持久化设置，但是在这种情况下，Redis 重启时会使用 AOF 文件来重建数据集，因为 AOF 文件保存的数据往往更加完整

### 2.2.2. 详解RDB

#### 2.2.2.1. RDB的创建与载入

Redis 提供了 ***SAVE\*** 和 ***BGSAVE\*** 两个命令来生成 RDB 文件，区别是前者是阻塞的，后者是后台 ***fork\*** 子进程进行不会阻塞主进程处理命令请求。载入 RDB 文件不需要手工运行，而是 ***server\*** 端自动进行，只要启动时检测到 RDB 文件存在 ***server\*** 端便会载入 RDB 文件重建数据集。当然上面简介中已经提到，如果 同时存在 AOF 的话会优先使用 AOF 重建数据集因为其保存的数据更完整。

```java
void saveCommand(client *c) {
    // BGSAVE执行时不能执行SAVE
    if (server.rdb_child_pid != -1) {
        addReplyError(c,"Background save already in progress");
        return;
    }
    rdbSaveInfo rsi, *rsiptr;
    rsiptr = rdbPopulateSaveInfo(&rsi);
    // 调用rdbSave函数执行备份（阻塞当前客户端）
    if (rdbSave(server.rdb_filename,rsiptr) == C_OK) {
        addReply(c,shared.ok);
    } else {
        addReply(c,shared.err);
    }
}

/*
* BGSAVE 命令实现 [可选参数"schedule"]
*/
void bgsaveCommand(client *c) {
    int schedule = 0;

    /* 当AOF正在执行时，SCHEDULE参数修改BGSAVE的效果
    * BGSAVE会在之后执行，而不是报错
    * 可以理解为：BGSAVE被提上日程
    */
    if (c->argc > 1) {
        // 参数只能是"schedule"
        if (c->argc == 2 && !strcasecmp(c->argv[1]->ptr,"schedule")) {
            schedule = 1;
        } else {
            addReply(c,shared.syntaxerr);
            return;
        }
    }

    // BGSAVE正在执行，不操作
    if (server.rdb_child_pid != -1) {
        addReplyError(c,"Background save already in progress");
    } else if (server.aof_child_pid != -1) {
        // aof正在执行，如果schedule==1，BGSAVE被提上日程
        if (schedule) {
            server.rdb_bgsave_scheduled = 1;
            addReplyStatus(c,"Background saving scheduled");
        } else {
            addReplyError(c,
            "An AOF log rewriting in progress: can't BGSAVE right now. "
            "Use BGSAVE SCHEDULE in order to schedule a BGSAVE whenever "
            "possible.");
        }
    } else if (rdbSaveBackground(server.rdb_filename,NULL) == C_OK) {// 否则调用rdbSaveBackground执行备份操作
        addReplyStatus(c,"Background saving started");
    } else {
        addReply(c,shared.err);
    }
}
```

#### 2.2.2.2. RDB 相关配置

**SAVE POINT** *save <seconds> <changes>*

你可以配置**保存点(*****save point\*****)**，Redis 如果每 ***N\*** 秒数据发生了 ***M\*** 次改变就保存快照文件，例如下面：

```java
# 这个保存点配置表示每60秒，如果数据发生了1000次以上的变动，Redis就会自动保存快照文件
save 60 1000
# 保存点可以设置多个，Redis的配置文件就默认设置了3个保存点
# 格式为：save <seconds> <changes>
# 可以设置多个。
save 900 1 #900秒后至少1个key有变动
save 300 10 #300秒后至少10个key有变动
save 60 10000 #60秒后至少10000个key有变动
```

**stop-writes-on-bgsave-error** *yes | no*

如果 Redis 执行 RDB 持久化失败（常见于操作系统内存不足），那么 Redis 将不再接受 client 写入数据的请求。当然在实践中，我们通常会将 ***stop-writes-on-bgsave-error\*** 设置为 ***false\***，同时让监控系统在 Redis 执行 RDB 持久化失败时发送告警，以便介入解决，而不是粗暴地拒绝 client 的写入请求。

**rdbcompression** *yes | no*

当生成 RDB 文件时，Redis 会判断字符串长度 >=20字节则压缩，否则不压缩存储，默认 Redis 会采用 [LZF](https://github.com/ning/compress) 算法进行数据压缩。

**rdbchecksum** *yes | no*

从版本5的 RDB 的开始，一个 **CRC64** 的校验码会放在文件的末尾。这样更能保证文件的完整性，但是在保存或者加载文件时会损失一定的性能（大概10%）。如果想追求更高的性能，可以把它禁用掉，这样文件在写入校验码时会用 **0** 替代，加载的时候看到 **0** 就会直接跳过校验。

#### 2.2.2.3. RDB 的优点

- RDB文件是一个很简洁的单文件，它保存了某个时间点的 Redis 数据集，很适合用于做备份。你可以设定一个时间点对 RDB 文件进行归档，这样就能在需要的时候很轻易的把数据恢复到不同的版本。
- 基于上面所描述的特性，RDB 文件很适合用于灾备，因为单文件可以很方便地传输到另外的数据中心。
- RDB的性能很好，需要进行持久化时，主进程会 ***fork\*** 一个子进程出来，然后把持久化的工作交给子进程，自己不会有相关的 I/O 操作。
- 比起 AOF，在数据量比较大的情况下，RDB的启动速度更快。

#### 2.2.2.4. RDB 的缺点

- RDB容易造成数据的丢失，当你希望在 Redis 停止工作时尽量减少数据丢失的话，那 RDB 不适用。假设每5分钟保存一次快照，如果Redis因为某些原因不能正常工作，那么从上次产生快照到 Redis 出现问题这段时间的数据就会丢失了。你可以通过配置不同的 ***save point\*** 来减轻数据丢失的程度，但是越紧凑的 ***save point\*** 会越频繁地触发 RDB 生成操作，从而对 Redis 性能产生影响
- RDB 使用 ***fork\*** 子进程进行数据的持久化，如果数据比较大的话可能就会花费点时间，造成 Redis 停止服务几毫秒，如果数据量很大且 CPU 性能不是很好的时候，停止服务的时间甚至会到一秒。AOF 也需要 ***fork\***但是你可以自己调整 rewrite 的频率，它不会造成数据丢失。在 Linux 系统中，***fork\*** 会拷贝进程的 ***page table\***。随着进程占用的内存越大，进程的 ***page table\*** 也会越大，那么 ***fork\*** 也会占用更多的时间。 如果 Redis 占用的内存很大 (例如 20 GB)，那么在 ***fork\*** 子进程时，会出现明显的停顿现象（无法处理 client 的请求）。另外，在不同机器上，fork 的性能是不同的，可以参见 [Fork time in different systems](https://redis.io/topics/latency#fork-time-in-different-systems)
- Linux ***fork\*** 子进程采用的是 ***copy-on-write\*** 的方式。在 Redis 执行 RDB 持久化期间，如果 client 写入数据很频繁，那么将增加 Redis 占用的内存，最坏情况下，内存的占用将达到原先的两倍。

### 2.2.3. 3. 详解 AOF

#### 2.2.3.1. AOF 实现

和 RDB 持久化数据库键值对来记录数据库状态不同，AOF 是通过保存对数据库的写命令集来记录数据库状态的。AOF 持久化实现可以分为**命令追加(append)、文件写入(write)、文件同步(fsync)** 三个步骤。***Append\*** 追加命令到 AOF 缓冲区，***Write\*** 将缓冲区的内容写入到程序缓冲区，***Fsync\*** 将程序缓冲区的内容写入到文件。 

**命令追加**

当 AOF 持久化功能打开时，***server\*** 端每执行完一个写命令，会以协议格式将被执行的写命令追加到 ***server\*** 端 ***redisServer\*** 结构体中的 ***aof_buf\*** 缓冲区末尾。

**文件写入与同步**

Redis server 进程是一个**事件循环(event loop)**，***server\*** 每结束一个事件循环之前都会调用 ***flushAppendOnlyFile\*** 函数，考虑是否将 ***aof_buf\*** 缓冲区中的内容吸入和保存到 AOF 文件，而 ***flushAppendOnlyFile\*** 函数的行为由 ***appendfsync\*** 选项来控制

| appendfsync 值 | flushAppendOnlyFile 行为                                     |
| -------------- | ------------------------------------------------------------ |
| always         | 每个事件循环都将 aof_buf 缓冲区中的内容写入 AOF 文件，并且调用 fsync() 将其同步到磁盘。这可以保证最好的数据持久性，但却会给系统带来极大的开销，其效率是三者中最慢的，但同时安全性也是最高的，即使宕机也只丢失一个事件循环中的数据。 |
| no             | 每个事件循环都将 aof_buf 缓冲区中的内容写入 AOF 文件，但不对其进行同步，何时同步至磁盘会让操作系统决定。这种模式下 AOF 的写入速度最快，不过因其会在系统缓存中积累一段时间的数据，所以同步时间为三者最长。一旦宕机将会丢失自上一次同步 AOF 文件起所有的数据。 |
| everysec       | 每个事件循环都将 aof_buf 缓冲区中的内容写入 AOF 文件，Redis 还会每秒在子线程中执行一次 fsync()。在实践中，推荐使用这种设置，一定程度上可以保证数据持久性，又不会明显降低 Redis 性能。 |

#### 2.2.3.2. AOF 重写

AOF 持久化并不是没有缺点的，Redis 会不断将接收到的写命令追加到 AOF 文件中，导致 AOF 文件越来越大。过大的 AOF 文件会消耗磁盘空间，并且导致 Redis 重启时更加缓慢。为了解决这个问题，在适当情况下，Redis 会对 AOF 文件进行重写，去除文件中冗余的命令，以减小 AOF 文件的体积。

AOF的重写会执行大量的写入操作，Redis是单线程的，所以如果有服务器直接调用重写，服务器就不能处理其他命令了，因此Redis服务器新起了单独一个进程来执行AOF重写。当然也可以通过 ***BGREWRITEAOF\*** 命令手动重写 AOF 文件。重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。

在子进程执行AOF重写时，服务端接收到客户端的命令之后，先执行客户端发来的命令，然后将执行后的写命令追加到AOF缓冲区中，同时将执行后的写命令追加到AOF重写缓冲区中。 等到子进程完成了重写工作后，会发一个完成的信号给服务器，服务器就将AOF重写缓冲区中的所有内容追加到AOF文件中，然后原子性地覆盖现有的AOF文件。



![img](https://cdn.nlark.com/yuque/0/2024/jpeg/42819892/1716283619565-34a23629-6540-4f32-b44b-8bcfaa30f65d.jpeg)

#### 2.2.3.3. AOF相关配置

```java
#你可以在 redis.conf 中通过以下配置开启 AOF 功能
appendonly yes

# 文件存放目录，与RDB共用。默认为当前工作目录。
dir ./

# 默认文件名为appendonly.aof
appendfilename "appendonly.aof"

# fsync 相关配置
# appendfsync always
appendfsync everysec
# appendfsync no

# Redis会记住自从上一次重写后AOF文件的大小（如果自Redis启动后还没重写过，则记住启动时使用的AOF文件的大小）。
# 如果当前的文件大小比起记住的那个大小超过指定的百分比，则会触发重写。
# 同时需要设置一个文件大小最小值，只有大于这个值文件才会重写，以防文件很小，但是已经达到百分比的情况。
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
# 上面两个配置的作用：当 AOF 文件的体积大于 64MB，并且 AOF 文件的体积比上一次重写之后的体积大了至少一倍，那么 Redis 就会执行 AOF 重写。

# 要禁用自动的日志重写功能，我们可以把百分比设置为0：
auto-aof-rewrite-percentage 0
```

#### 2.2.3.4. AOF 的优点

- 比RDB可靠。你可以制定不同的 ***fsync\*** 策略：***no\***、***everysec\*** 和 ***always\***。默认是 ***everysec\***。这意味着你最多丢失一秒钟的数据。
- AOF日志文件是一个纯追加的文件。就算是遇到突然停电的情况，也不会出现日志的定位或者损坏问题。甚至如果因为某些原因（例如磁盘满了）命令只写了一半到日志文件里，我们也可以用 ***redis-check-aof\*** 这个工具很简单的进行修复。
- 当AOF文件太大时，Redis 会自动在后台进行重写。重写很安全，因为重写是在一个新的文件上进行，同时 Redis 会继续往旧的文件追加数据。新文件上会写入能重建当前数据集的最小操作命令的集合。当新文件重写完，Redis 会把新旧文件进行切换，然后开始把数据写到新文件上。
- AOF 把操作命令以简单易懂的格式一条接一条的保存在文件里，很容易导出来用于恢复数据。例如我们不小心用 ***FLUSHALL\*** 命令把所有数据刷掉了，只要文件没有被重写，我们可以把服务停掉，把最后那条命令删掉，然后重启服务，这样就能把被刷掉的数据恢复回来。

#### 2.2.3.5. AOF 的缺点

- 在相同的数据集下，AOF 文件的大小一般会比 RDB 文件大。
- 在某些 ***fsync\*** 策略下，AOF 的速度会比 RDB 慢。通常 ***fsync\*** 设置为每秒一次就能获得比较高的性能，而在禁止 ***fsync\*** 的情况下速度可以达到 RDB 的水平。
- 在过去曾经发现一些很罕见的BUG导致使用AOF重建的数据跟原数据不一致的问题

### 2.2.4. Redis 4.0混合持久化

Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 ***aof-use-rdb-preamble\*** 开启）。如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。

## 2.3. 主从复制

### 2.3.1. **什么是主从复制**

#### 2.3.1.1. 简介

在分布式环境中，数据副本 ***(Replica)\*** 和复制 ***(Replication)\*** 作为提升系统可用性和读写性能的有效手段被大量应用系统设计中，Redis 也不例外。Redis 作为单机数据库使用时，适用常见有限且存在单点宕机问题，无法维持高可用。因此 Redis 允许通过 ***SLAVEOF\*** 命令或者 ***slaveof\*** 配置项来让一个 Redis server 复制另一个 Redis server 的数据集和状态，我们称之为主从复制，主服务器下文称 ***master\***，从服务器下文称 ***slave，\***Redis 采用异步的复制机制。

复制机制的运行依靠三个特性：

1. 当一个 ***master\*** 和一个 ***slave\*** 连接正常时，***master\*** 会发送一连串的命令流来保持对 ***slave\*** 的更新，以便于将自身数据集的变更复制给 ***slave\*** ：包括客户端的写入、***key\*** 的过期或被逐出等
2. 当 ***master\*** 和 ***slave\*** 之间的连接断开后（断开的原因可能是网络问题或者连接超时） ***slave\*** 重连上 ***master\*** 并尝试进行部分重同步，这意味着它只会尝试获取在断开连接期间内丢失的命令流
3. 当无法进行部分重同步时， ***slave\*** 会请求进行全量重同步。这会涉及到一个更复杂的过程，例如 ***master\*** 需要创建所有数据的快照，将之发送给 ***slave\*** ，之后在数据集更改时持续发送命令流到 ***slave\***

#### 2.3.1.2. **主从复制的优点**

- ***master\*** 可以关闭持久化机制，减少不必要的 IO 操作且降低延迟，对于以性能著称的组件来说极为重要
- ***slave\*** 虽然不能处理写请求，但是可以处理读请求，从而增加读取操作的吞吐量。但由于复制机制的原因，主从数据存在不一致的时间窗口
- 使得 Redis 可以告别单机版本的单点风险，采用副本形式提高可用性，在 ***master\*** 宕机时可以将 ***slave\*** 提升为 ***master\*** 继续向外提供服务，也为 Redis 集群模式的诞生奠定了技术基础

这里需要注意的是 Redis 2.8 版本之前与之后采用的复制方式不尽相同，主要区别在将成本极高的 ***sync\*** 替换为 ***psync\***，增加了断线重连情况下根据主从保存的 ***offset\*** 即复制偏移量进行增量同步的功能，考虑到目前 Squirrel 线上集群绝大部分收敛至 3.2.8 版本，因此本文不再赘述旧版复制机制。

#### 2.3.1.3. **1.3 主从复制和集群**

有时我们会混淆这两个概念，主从复制也是采用了多个 Redis 节点，和 Redis 集群表面上看很接近，那二者究竟有什么区别呢？

| **Replication** | 复制机制中包含了一个 ***master\*** 和若干个 ***slave\***，其中写请求只能 ***master\*** 来处理，数据的变更转化为数据流异步发送给 ***slaves\*** 进行更新；读请求则可以根据使用场景来规定是否由 ***slave\*** 处理从而增加系统的读吞吐量。一旦 ***master\*** 发生故障，***slave\*** 可以被提升为 ***master\*** 从而继续提供服务。因此总结起来，***slave\*** 在复制机制的场景下，可以提供故障恢复、分担读流量和数据备份的功能。 |
| --------------- | ------------------------------------------------------------ |
| **Cluster**     | 集群机制的使用意味着你的数据量较大，数据会根据 ***Key\*** 计算出的 ***slot\*** 值自动在多个分片上进行分区***(Partitioning)\***，客户端对某个 ***Key\*** 的请求会被转发到持有那个 ***Key\*** 的分片上。分片由一个 ***master\*** 和若干个 ***slave\*** 组成，二者间通过复制机制同步数据。因此总结来看，集群模式更像分区和复制机制的组合。 |

### 2.3.2. 如何开启主从复制

需要注意，主从复制的开启，完全是在 ***slave\*** 发起的；不需要我们在 ***master\*** 做任何事情。***slave\*** 开启主从复制，有三种方式：

```shell
#配置文件，在从服务器的配置文件中加入
slaveof<masterip><masterport>
#启动命令 Redis server启动命令后加入
--slaveof<masterip><masterport>
#客户端命令 Redis server启动命令后直接通过客户端执行命令
slaveof<masterip><masterport> 则该redis实例变为slave
```

### 2.3.3. 主从复制机制的演变

从 Redis 2.6 到 4.0 开发人员对复制流程进行逐步的优化，以下是演进过程：

- 2.8 版本之前 Redis 复制采用 ***sync\*** 命令，无论是第一次主从复制还是断线重连后再进行复制都采用全量同步，成本高
- 2.8 ~ 4.0 之间复制采用 ***psync\*** 命令，这一特性主要添加了 Redis 在断线重连时候可通过 offset 信息使用部分同步
- 4.0 版本之后也采用 ***psync\***，相比于 2.8 版本的 ***psync\*** 优化了增量复制，这里我们称为 ***psync2\***，2.8 版本的 psync 可以称为 ***psync1\***

我们先介绍 ***psync1\*** 和 ***psync2\*** 通用的复制原理，然后再细谈二者的区别和优化点，至于旧版 ***sync\*** 的机制本文不再赘述。

### 2.3.4. 主从复制的原理

主从复制过程可分为三个阶段：复制初始化、数据同步和命令传播。

#### 2.3.4.1. 复制初始化阶段

当执行完 ***slaveof\*** 命令后，***slave\*** 根据指明的 ***master\*** 地址向 ***master\*** 发起 socket 连接，***master\*** 收到 socket 连接之后将连接信息保存，此时连接建立完成；当 socket 连接建立完成以后，***slave\*** 向 ***master\*** 发送 ***PING\*** 命令，以确认 ***master\*** 是否存活，此时的结果返回如果是 ***PONG\*** 则代表 ***master\*** 可用，否则可能出现超时或者 ***master\*** 此时在处理其他任务阻塞了，那么此时 ***slave\*** 将断开 socket 连接，然后进行重试；

如果 ***master\*** 连接设置了密码，则 ***slave\*** 需要设置 ***masterauth\*** 参数，此时 ***slave\*** 会发送 ***auth\*** 命令，命令格式为 ***auth + 密码\*** 进行密码验证，其中密码为 ***masterauth\*** 参数配置的密码，需要注意的是如果 ***master\*** 设置了密码验证，从库未配置 ***masterauth\*** 参数则会报错，socket 连接断开。当身份验证完成以后，***slave\*** 发送自己的监听端口，***master\*** 保存其端口信息，此时进入下一个阶段：数据同步阶段。

#### 2.3.4.2. 数据同步阶段

***master\*** 和 ***slave\*** 都确认对方信息以后，便可开始数据同步，此时 ***slave\*** 向主库发送 ***psync\*** 命令（需要注意的是 redis 4.0 对 2.8 版本的 ***psync\*** 做了优化），主库收到该命令后判断是进行增量同步还是全量同步，然后根据策略进行数据的同步，当 ***master\*** 有新的写操作时候，此时进入复制第三阶段：命令传播阶段。

#### 2.3.4.3. 命令传播阶段

当数据同步完成以后，在此后的时间里 ***master-slave\*** 之间维护着心跳检查来确认对方是否在线，每隔一段时间（默认10秒，通过 ***repl-ping-slave-period\*** 参数指定）***master\*** 向 ***slave\*** 发送 ***PING\*** 命令判断 ***slave\*** 是否在线，而 ***slave\***每秒一次向 ***master\*** 发送 ***REPLCONF ACK\*** 命令**，**命令格式为：REPLCONF ACK {offset} ，其中 ***offset\*** 指 ***slave\*** 保存的复制偏移量，作用有：

- 汇报自己复制偏移量，***master\*** 会对比复制偏移量向 ***slave\*** 发送未同步的命令
- 判断 ***master\*** 是否在线

***slave\*** 接送命令并执行，最终实现与主库数据相同

### 2.3.5. PSYNC1 和 PSYNC2

#### 2.3.5.1. PSYNC1

为了解决旧版 ***SYNC\*** 在处理断线重连复制场景下的低效问题，Redis 2.8 采用 ***PSYNC\*** 代替 ***SYNC 命令。PSYNC\*** 命令具有全量同步和部分同步两种模式。

##### 2.3.5.1.1. **全量重同步**

前者和 SYNC 大致相同，都是让 ***master\*** 生成并发送 RDB 文件，然后再将保存在缓冲区中的写命令传播给 ***slave\***来进行同步，相当于只有同步和命令传播两个阶段。

##### 2.3.5.1.2. **部分重同步**

部分同步适用于断线重连之后的同步，***slave\*** 只需要接收断线期间丢失的写命令就可以，不需要进行全量同步。为了实现部分同步，引入了复制偏移量***（offset）\***、复制积压缓冲区***（replication backlog buffer）\***和运行 ID ***（run_id）\***三个概念。

| **复制偏移量**     | 执行主从复制的双方都会分别维护一个复制偏移量，***master\*** 每次向 ***slave\*** 传播 ***N\*** 个字节，自己的复制偏移量就增加 ***N\***；同理 ***slave\*** 接收 ***N\*** 个字节，自身的复制偏移量也增加 ***N\***。通过对比主从之间的复制偏移量就可以知道主从间的同步状态。 |
| ------------------ | ------------------------------------------------------------ |
| **复制积压缓冲区** | 复制积压缓冲区是 ***master\*** 维护的一个固定长度的 FIFO 队列，默认大小为 1MB。当 ***master\*** 进行命令传播时，不仅将写命令发给 ***slave\*** 还会同时写进复制积压缓冲区，因此 ***master\*** 的复制积压缓冲区会保存一部分最近传播的写命令。当 ***slave\*** 重连上 ***master\*** 时会将自己的复制偏移量通过 ***PSYNC\*** 命令发给 ***master\***，***master\*** 检查自己的复制积压缓冲区，如果发现这部分未同步的命令还在自己的复制积压缓冲区中的话就可以利用这些保存的命令进行部分同步，反之如果断线太久这部分命令已经不在复制缓冲区中了，那没办法只能进行全量同步。 |
| **运行 ID**        | 令人疑惑的是上述逻辑看似已经很圆满了，这个 ***run_id\*** 是做什么用呢？其实这是因为 ***master\*** 可能会在 ***slave\***断线期间发生变更，例如可能超时失去联系或者宕机导致断线重连的是一个崭新的 ***master\***，不再是断线前复制的那个了。自然崭新的 ***master\*** 没有之前维护的复制积压缓冲区，只能进行全量同步。因此每个 Redis server 都会有自己的运行 ID，由 40 个随机的十六进制字符组成。当 ***slave\*** 初次复制 ***master\*** 时，***master\*** 会将自己的运行 ID 发给 ***slave\*** 进行保存，这样 ***slave\***重连时再将这个运行 ID 发送给重连上的 ***master\*** ，***master\*** 会接受这个 ID 并于自身的运行 ID 比较进而判断是否是同一个 ***master\***。 |

##### 2.3.5.1.3. PSYNC 1流程图

![img](https://cdn.nlark.com/yuque/0/2024/jpeg/42819892/1716285404191-5adf2021-0543-496d-8da1-c115a467804e.jpeg)

如果 ***slave\*** 以前没有复制过任何 ***master\***，或者之前执行过 SLAVEOF NO ONE 命令，那么 ***slave\*** 在开始一次新的复制时将向主服务器发送 PSYNC ? -1 命令，主动请求 ***master\*** 进行完整重同步（因为这时不可能执行部分重同步）。相反地，如果 ***slave\*** 已经复制过某个 ***master\***，那么 ***slave\*** 在开始一次新的复制时将向 ***master\*** 发送 PSYNC <runid> <offset> 命令：其中 runid 是上一次复制的 ***master\*** 的运行ID，而 ***offset\*** 则是 ***slave\*** 当前的复制偏移量，接收到这个命令的 ***master\*** 会通过这两个参数来判断应该对 ***slave\*** 执行哪种同步操作。

根据情况，接收到 ***PSYNC\*** 命令的 ***master\*** 会向 ***slave\*** 返回以下三种回复的其中一种：

- 如果 ***master\*** 返回 +FULLRESYNC <runid> <offset> 回复，那么表示 ***master\*** 将与 ***slave\*** 执行完整重同步操作：其中 runid 是这个 ***master\*** 的运行 ID，***slave\*** 会将这个 ID 保存起来，在下一次发送 PSYNC 命令时使用；而 ***offset\*** 则是 ***master\*** 当前的复制偏移量，***slave\*** 会将这个值作为自己的初始化偏移量
- 如果 ***master\*** 返回 +CONTINUE 回复，那么表示 ***master\*** 将与 ***slave\*** 执行部分同步操作，***slave\*** 只要等着 ***master\*** 将自己缺少的那部分数据发送过来就可以了
- 如果 ***master\*** 返回 -ERR 回复，那么表示 ***master\*** 的版本低于 Redis 2.8，它识别不了 ***psync\*** 命令，***slave\*** 将向 ***master\*** 发送 ***SYNC\*** 命令，并与 ***master\*** 执行完整同步操作

由此可见 ***psync\*** 也有不足之处，当 ***slave\*** 重启以后 ***master\*** runid 发生变化，也就意味者 ***slave\*** 还是会进行全量复制，而在实际的生产中进行 ***slave\*** 的维护很多时候会进行重启，而正是有由于全量同步需要 ***master\*** 执行快照，以及数据传输会带不小的影响。因此在 4.0 版本，***psync\*** 命令做了改进，我们称之为 ***psync2\***。

#### 2.3.5.2.  PSYNC2

Redis 4.0 版本新增混合持久化，还优化了***psync\***（以下称 ***psync2\***），***psync2\*** 最大的变化支持两种场景下的部分重同步，一个场景是 ***slave\*** 提升为 ***master\*** 后，其他 ***slave\*** 可以从新提升的 ***master\*** 进行部分重同步，这里需要 ***slave\***默认开启复制积压缓冲区；另外一个场景就是 ***slave\*** 重启后，可以进行部分重同步。这里要注意和 ***psync1\*** 的运行 ID 相比，这里的复制 ID 有不一样的意义。

##### 2.3.5.2.1. **优化细节**

Redis 4.0 引入另外一个变量 ***master_replid 2\*** 来存放同步过的 ***master\*** 的复制 ID，同时复制 ID 在 ***slave\*** 上的意义不同于之前的运行 ID，复制 ID 在 ***master\*** 的意义和之前运行 ID 仍然是一样的，但对于 ***slave\*** 来说，它保存的复制 ID（即 replid） 表示当前正在同步的 ***master\*** 的复制 ID 。***master_replid 2\*** 则表示前一个 ***master\*** 的复制 ID（如果它之前没复制过其他的 ***master\***，那这个字段没用），这个在主从角色发生改变的时候会用到。

***slave\*** 在意外关闭前会调用 ***rdbSaveInfoAuxFields\*** 函数把当前的复制 ID（即关闭前正在复制的 ***master\*** 的 replid，因为 ***slave\*** 中的 replid 字段保存的是 ***master\*** 的复制 ID） 和复制偏移量一起保存到 RDB 文件中，后面该 ***slave\*** 重启的时候，就可以从 RDB 文件中读取复制 ID 和复制偏移量，然后重连上 ***master\*** 后 ***slave\*** 将这两个值发送给 ***master，master\*** 会如下判断是否允许 ***psync\***：

```java
//如果salve发送过来的复制ID是当前master的复制ID，说明master没变过
//或者和现在的新master曾属于同一个master
//但是同步进度不能比当前master还快
{
    ....
}

//判断同步进度是否已经超过范围
....
```

另外当节点从 ***slave\*** 提升为 ***master\*** 后，会保存两个复制 ID（之前角色是 ***slave\*** 的时候 ***replid2\*** 没用，现在要派上用场了），分别是 ***replid\*** 和 ***replid 2\***，其他 ***slave\*** 复制的时候可以根据第二个复制 ID 来进行部分重同步。

## 2.4. 集群模式

### 2.4.1. **Redis Cluster**

#### 2.4.1.1. Redis 集群方案的演变

大规模数据存储系统都会面临的一个问题就是如何横向拓展。当你的数据集越来越大，一主多从的模式已经无法支撑这么大量的数据存储，于是你首先考虑将多个主从模式结合在一起对外提供服务，但是这里有两个问题就是如何实现数据分片的逻辑和在哪里实现这部分逻辑？业界常见的解决方案有两种，一是引入 ***Proxy\*** 层来向应用端屏蔽身后的集群分布，客户端可以借助 ***Proxy\*** 层来进行请求转发和 Key 值的散列从而进行进行数据分片，这种方案会损失部分性能但是迁移升级等运维操作都很方便，业界 ***Proxy\*** 方案的代表有 Twitter 的 [Twemproxy](https://github.com/twitter/twemproxy) 和豌豆荚的 [Codis](https://github.com/CodisLabs/codis)；二是 ***smart client\*** 方案，即将 ***Proxy\*** 的逻辑放在客户端做，客户端根据维护的映射规则和路由表直接访问特定的 Redis 实例，但是增减 Redis 实例都需要重新调整分片逻辑。

#### 2.4.1.2. Redis Cluster简介

Redis 3.0 版本开始官方正式支持集群模式，Redis 集群模式提供了一种能将数据在多个节点上进行分区存储的方法，采取了和上述两者不同的实现方案——去中心化的集群模式，集群通过分片进行数据共享，分片内采用一主多从的形式进行副本复制，并提供复制和故障恢复功能。在官方文档 [Redis Cluster Specification](https://redis.io/topics/cluster-spec) 中，作者详细介绍了官方集群模式的设计考量，主要有以下几点：

| **性能**     | Redis 集群模式采用去中心化的设计，即 P2P 而非之前业界衍生出的 Proxy 方式 |
| ------------ | ------------------------------------------------------------ |
| **一致性**   | ***master\*** 与 ***slave\*** 之间采用异步复制，存在数据不一致的时间窗口，保证高性能的同时牺牲了部分一致性 |
| **水平扩展** | 文中称可以线性扩展至 1000 个节点                             |
| **可用性**   | 在集群模式推出之前，主从模式的可用性要靠 ***Sentinel\*** 保证，集群模式引入了新的故障检测机制，而在故障转移这块复用了 ***Sentinel\*** 的代码逻辑，不需要单独启动一个 Sentinel 集群，Redis Cluster本身就能自动进行 ***master\*** 选举和 ***failover\*** |

下图是一个三主三从的 Redis Cluster，三机房部署（其中一主一从构成一个分片，之间通过异步复制同步数据，一旦某个机房掉线，则分片上位于另一个机房的 ***slave\*** 会被提升为 ***master\*** 从而可以继续提供服务） ；每个 ***master\*** 负责一部分 ***slot\***，数目尽量均摊；客户端对于某个 ***Key\*** 操作先通过公式计算（计算方法见下文）出所映射到的 ***slot\***，然后直连某个分片，写请求一律走 ***master\***，读请求根据路由规则选择连接的分片节点。



![img](https://cdn.nlark.com/yuque/0/2024/jpeg/42819892/1716287155681-8b6d9baf-3e8f-4b68-9e94-3fca50f80fd2.jpeg)

#### 2.4.1.3. 三种集群方案的优缺点

| 集群模式      | 优点                                                         | 缺点                                                         |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 客户端分片    | 不使用第三方中间件，实现方法和代码可以自己掌控并且可随时调整。这种分片性能比代理式更好(因为少了分发环节)，分发压力在客户端，无服务端压力增加 | 不能平滑地水平扩容，扩容/缩容时，必须手动调整分片程序，出现故障不能自动转移，难以运维 |
| 代理层分片    | 运维成本低。业务方不用关心后端 Redis 实例，跟操作单点 Redis 实例一样。Proxy 的逻辑和存储的逻辑是隔离的 | 代理层多了一次转发，性能有所损耗；进行扩容/缩容时候，部分数据可能会失效，需要手动进行迁移，对运维要求较高，而且难以做到平滑的扩缩容；出现故障，不能自动转移，运维性很差。Codis 做了诸多改进，相比于 [Twemproxy](https://github.com/twitter/twemproxy) 可用性和性能都好得多 |
| Redis Cluster | 无中心节点，数据按照 slot 存储分布在多个 Redis 实例上，平滑的进行扩容/缩容节点，自动故障转移（节点之间通过 Gossip 协议交换状态信息,进行投票机制完成 slave 到 master 角色的提升）降低运维成本，提高了系统的可扩展性和高可用性 | 开源版本缺乏监控管理，原生客户端太过简陋，failover 节点的检测过慢，维护 Membership 的 Gossip消息协议开销大，无法根据统计区分冷热数据 |

### 2.4.2.  哈希槽

#### 2.4.2.1. 什么是哈希槽

Redis Cluster 中，数据分片借助哈希槽 ***(下文均称 slot)** 来实现，集群预先划分 16384 个 **slot**，对于每个请求集群的键值对，根据 ***Key\*** 进行散列生成的值唯一匹配一个 ***slot\***。Redis Cluster 中每个分片的 ***master\*** 负责 16384 个 ***slot\*** 中的一部分，当且仅当每个 ***slot\*** 都有对应负责的节点时，集群才进入可用状态。当动态添加或减少节点时，需要将 16384 个 ***slot\*** 做个再分配，***slot\*** 中的键值也要迁移。

#### 2.4.2.2. 哈希计算方法

```java
HAS_SOLT = CRC16(key) mod 16384
```

但是上述计算方法实际采用时，做了一些改变，改变的目的是为了支持哈希标签***(Hash Tag)\***。哈希标签是确保两个键都在同一个 ***slot\*** 里的一种方式。为了实现哈希标签，***slot\*** 是用另一种不同的方式计算的。简单来说，如果一个键包含一个 “{…}” 这样的模式，只有 { 和 } 之间的字符串会被用来做哈希以获取 ***slot\***。但是由于可能出现多个 { 或 }，计算的算法如下：

```java
def HASH_SLOT(key)
    s = key.index "{"
    if s
        e = key.index "}",s+1
        if e && e != s+1
            key = key[s+1..e-1]
        end
    end
    crc16(key) % 16384
end
```

#### 2.4.2.3.  哈希槽的内部实现

Redis 集群中每个节点都会维护集群中所有节点的 ***clusterNode\*** 结构体，其中的 ***slots\*** 属性是个二进制位数组，长度为 2048 bytes，共包含 16384 个 ***bit\*** 位，节点可以根据某个 ***bit\*** 的 0/1 值判断对应的 ***slot\*** 是否由当前节点处理。每个节点通过 ***clusterStats\*** 结构体来保存从自身视角看去的集群状态，其中 ***nodes\*** 属性是一个保存节点名称和 ***clusterNode\*** 指针的字典，而 ***slots\*** 数组是一个记录哪个 ***slot\*** 属于哪个 ***clusterNode\*** 结构体的数组。



![img](https://cdn.nlark.com/yuque/0/2024/jpeg/42819892/1716288330124-f49536d2-9a34-4c86-9799-6ad7e33a7508.jpeg)

#### 2.4.2.4. 哈希槽的迁移

线上集群因为扩容和缩容操作，经常需要迁移 ***slot\*** 对数据进行重新分片，原生的 Redis Cluster 可以借助 [redis-trib](http://download.redis.io/redis-stable/src/redis-trib.rb)工具进行迁移。Squirrel 使用自研的 [Squirrel migrate](https://km.sankuai.com/page/28311454) 进行数据迁移和分片 ***rebalance。\***

***slot\*** 在迁移过程有两个状态，在迁出节点会对该 ***slot\*** 标记为 ***MIGRATING\***，在迁入节点会对该 ***slot\*** 标记为 ***IMPORTING。\***当该 ***slot\*** 内的 ***Key\*** 都迁移完毕之后，新的 ***slot\*** 归属信息都进过消息协议进行传播，最终集群中所有节点都会知道该 ***slot\*** 已经迁移到了目标节点，并更新自身保存的 slot 和节点间的映射关系。

### 2.4.3. 3. MOVED & ASK

***redis-cli\*** 是官方提供的客户端脚本，我们可以通过 ***redis-cli -c -p port\*** 命令连接任意一个 ***master\***，开始使用集群。

#### 2.4.3.1. 详解 MOVED

我们通过 ***redis-cli\*** 可以发起对集群的读写请求，节点会计算我们请求的 Key 所属的 ***slot\***，一旦发现该 ***slot\*** 并非由自己负责的话，会向客户端返回一个 ***MOVED\*** 错误（需要注意的是集群模式下 ***redis-cli\*** 不会打印 ***MOVED\*** 错误而是会直接显示 ***Redirected\***，使用单机版 ***redis-cli\*** 连接则可以看到 ***MOVED\*** 错误），指引客户端重定向到正确的节点，并再次发送先前的命令，得到正确的结果。

#### 2.4.3.2. 详解ASK

***MOVED\*** 意为这个 ***slot\*** 的负责已经永久转交给另一个节点，因此可以直接把请求准发给现在负责该 ***slot\*** 的节点。但是考虑在 ***slot\*** 迁移过程中，会出现属于该 ***slot\*** 的一部分 Key 已经迁移到目的地节点，而另一部分 Key 还在源节点，那如果这时收到了关于这个 ***slot\*** 的请求，那么源节点会先在自己的数据库里查找是否有这个 Key，查到的话说明还未迁移那么直接返回结果，查询失败的话就说明 Key 已经迁移到目的地节点，那么就向客户端返回一个 ***ASK\*** 错误，指引客户端转向目的地节点查询该 Key。同样该错误仅在单机版 redis-cli 连接时打印。

#### 2.4.3.3. 客户端处理

这两个错误在实际线上环境中出现频率很高，那么定制化的客户端如何处理这二者呢？如果客户端每次都随机连接一个节点然后利用 ***MOVED\*** 或者 ***ASK\*** 来重定向其实是很低效的，所以一般客户端会在启动时通过解析***CLUSTER NODES\*** 或者 ***CLUSTER SLOTS\*** 命令返回的结果得到 ***slot\*** 和节点的映射关系缓存在本地，一旦遇到 ***MOVED\*** 或者 ***ASK\*** 错误时会再次调用命令刷新本地路由（因为线上集群一旦出现 ***MOVED\*** 或者是 ***ASK\*** 往往是因为扩容分片导致数据迁移，涉及到许多 ***slot\*** 的重新分配而非单个，因此需要整体刷新一次），这样集群稳定时可以直接通过本地路由表迅速找到需要连接的节点。

### 2.4.4. 故障检测

跟大多数分布式系统一样，Redis Cluster 的节点间通过持续的 ***heart beat\*** 来保持信息同步，不过 Redis Cluster 节点信息同步是内部实现的，并不依赖第三方组件如 ***Zookeeper\***。集群中的节点持续交换 ***PING\***、***PONG\*** 数据，消息协议使用 [Gossip](https://en.wikipedia.org/wiki/Gossip_protocol)，这两种数据包的数据结构一样，之间通过 ***type\*** 字段进行区分。

Redis 集群中的每个节点都会定期向集群中的其他节点发送 ***PING\*** 消息，以此来检测对方是否存活，如果接收 ***PING\*** 消息的节点在规定时间内（***node_timeout\***）没有回复 ***PONG\*** 消息，那么之前向其发送 ***PING\*** 消息的节点就会将其标记为疑似下线状态（***PFAIL\***）。每次当节点对其他节点发送 ***PING\*** 命令的时候，它都会随机地广播三个它所知道的节点的信息，这些信息里面的其中一项就是说明节点是否已经被标记为 ***PFAIL\*** 或者 ***FAIL\***。当节点接收到其他节点发来的信息时，它会记下那些被集群中其他节点标记为 ***PFAIL\*** 的节点，这称为失效报告（***failure report\***）。如果节点已经将某个节点标记为 ***PFAIL\*** ，并且根据自身记录的失效报告显示，集群中的大部分 ***master\***也认为该节点进入了 ***PFAIL\*** 状态，那么它会进一步将那个失效的 ***master\*** 的状态标记为 ***FAIL\*** 。随后它会向集群广播 “该节点进一步被标记为 ***FAIL\*** ” 的这条消息，所有收到这条消息的节点都会更新自身保存的关于该 ***master\*** 节点的状态信息为 ***FAIL\***。

### 2.4.5. 5. 故障转移（Failover）

#### 2.4.5.1. 5.1 纪元（epoch）

Redis Cluster 使用了类似于 ***Raft\*** 算法 ***term\***（任期）的概念称为 ***epoch\***（纪元），用来给事件增加版本号。Redis 集群中的纪元主要是两种：***currentEpoch\*** 和 ***configEpoch\***。

**5.1.1 currentEpoch**

这是一个集群状态相关的概念，可以当做记录集群状态变更的递增版本号。每个集群节点，都会通过 server.cluster->currentEpoch 记录当前的 ***currentEpoch\***。

集群节点创建时，不管是 ***master\*** 还是 ***slave\***，都置 ***currentEpoch\*** 为 0。当前节点接收到来自其他节点的包时，如果发送者的 ***currentEpoch\***（消息头部会包含发送者的 ***currentEpoch\***）大于当前节点的***currentEpoch\***，那么当前节点会更新 ***currentEpoch\*** 为发送者的 ***currentEpoch\***。因此，集群中所有节点的 ***currentEpoch\*** 最终会达成一致，相当于对集群状态的认知达成了一致。

**5.1.2 currentEpoch 作用**

***currentEpoch\*** 作用在于，当集群的状态发生改变，某个节点为了执行一些动作需要寻求其他节点的同意时，就会增加 ***currentEpoch\*** 的值。目前 ***currentEpoch\*** 只用于 ***slave\*** 的故障转移流程，这就跟哨兵中的sentinel.current_epoch 作用是一模一样的。当 ***slave A\*** 发现其所属的 ***master\*** 下线时，就会试图发起故障转移流程。首先就是增加 ***currentEpoch\*** 的值，这个增加后的 ***currentEpoch\*** 是所有集群节点中最大的。然后***slave A\*** 向所有节点发起拉票请求，请求其他 ***master\*** 投票给自己，使自己能成为新的 ***master\***。其他节点收到包后，发现发送者的 ***currentEpoch\*** 比自己的 ***currentEpoch\*** 大，就会更新自己的 ***currentEpoch\***，并在尚未投票的情况下，投票给 ***slave A\***，表示同意使其成为新的 ***master\***。

**5.1.3 configEpoch**

这是一个集群节点配置相关的概念，每个集群节点都有自己独一无二的 configepoch。所谓的节点配置，实际上是指节点所负责的槽位信息。

每一个 ***master\*** 在向其他节点发送包时，都会附带其 ***configEpoch\*** 信息，以及一份表示它所负责的 ***slots\*** 信息。而 ***slave\*** 向其他节点发送包时，其包中的 ***configEpoch\*** 和负责槽位信息，是其 ***master\*** 的 ***configEpoch\*** 和负责的 ***slot\***信息。节点收到包之后，就会根据包中的 ***configEpoch\*** 和负责的 ***slots\*** 信息，记录到相应节点属性中。

**5.1.4 configEpoch 作用**

***configEpoch\*** 主要用于解决不同的节点的配置发生冲突的情况。举个例子就明白了：节点A 宣称负责 ***slot 1\***，其向外发送的包中，包含了自己的 ***configEpoch\*** 和负责的 ***slots\*** 信息。节点 C 收到 A 发来的包后，发现自己当前没有记录 ***slot 1\*** 的负责节点（也就是 server.cluster->slots[1] 为 NULL），就会将 A 置为 ***slot 1\*** 的负责节点（server.cluster->slots[1] = A），并记录节点 A 的 ***configEpoch\***。后来，节点 C 又收到了 B 发来的包，它也宣称负责 ***slot 1\***，此时，如何判断 ***slot 1\*** 到底由谁负责呢？

这就是 ***configEpoch\*** 起作用的时候了，C 在 B 发来的包中，发现它的 ***configEpoch\***，要比 A 的大，说明 B 是更新的配置。因此，就将 ***slot 1\*** 的负责节点设置为 B（server.cluster->slots[1] = B）。在 ***slave\*** 发起选举，获得足够多的选票之后，成功当选时，也就是 ***slave\*** 试图替代其已经下线的旧 ***master\***，成为新的 ***master\*** 时，会增加它自己的 ***configEpoch\***，使其成为当前所有集群节点的 ***configEpoch\*** 中的最大值。这样，该 ***slave\*** 成为 ***master\*** 后，就会向所有节点发送广播包，强制其他节点更新相关 ***slots\*** 的负责节点为自己。

#### 2.4.5.2. 5.1 自动 Failover

- 当一个 ***slave\*** 发现自己正在复制的 ***master\*** 进入了已下线（***FAIL\***）状态时，***slave\*** 将开始对已下线状态的 ***master\*** 进行故障转移，以下是故障转移执行的步骤
- 该下线的 ***master\*** 下所有 ***slave\*** 中，会有一个 ***slave\*** 被选中。具体的选举流程为：slave 自增它的 ***currentEpoch\*** 值，然后向其他 ***masters\*** 请求投票，每个 ***slave\*** 都向集群其他节点广播一条 CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST 消息用于拉票，集群中具有投票权的 ***master\*** 收到消息后，如果在当前选举纪元中没有投过票，就会向第一个发送来消息的 ***slave\*** 返回 CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK 消息，表示投票给该 ***slave\***。某个 ***slave\*** 如果在一段时间内收到了大部分 ***master\*** 的投票，则表示选举成功。
- 被选中的 ***slave\*** 会执行 SLAVEOF no one 命令，成为新的 ***master\***
- 新的 ***master\*** 会撤销所有对已下线 ***master\*** 的 ***slot\*** 指派，并将这些 ***slot\*** 全部指派给自己
- 新的 ***master\*** 向集群广播一条 ***PONG\*** 消息，这条 ***PONG\*** 消息可以让集群中的其他节点立即知道自己已经由 ***slave\*** 变成了 ***master\*** ，并且这个 ***master\*** 已经接管了原本由已下线节点负责处理的 ***slot\***
- 新的 ***master\*** 开始接收和自己负责处理的 ***slot\*** 有关的命令请求，故障转移完成

#### 2.4.5.3. 5.2 手动 Failover

Redis 集群支持手动故障转移，也就是向 ***slave\*** 发送 CLUSTER  FAILOVER 命令，使其在 master 未下线的情况下，发起故障转移流程，升级为新的 ***master\*** ，而原来的 ***master\*** 降级为 ***slave\***。

为了不丢失数据，向 ***slave\*** 发送 CLUSTER  FAILOVER 命令后，流程如下：

1. ***slave\*** 收到命令后，向 ***master\*** 发送 CLUSTERMSG_TYPE_MFSTART 命令
2. ***master\*** 收到该命令后，会将其所有客户端置于阻塞状态，也就是在 10s 的时间内，不再处理客户端发来的命令，并且在其发送的心跳包中，会带有 CLUSTERMSG_FLAG0_PAUSED 标记
3. ***slave\*** 收到 ***master\*** 发来的，带 CLUSTERMSG_FLAG0_PAUSED 标记的心跳包后，从中获取 ***master\*** 当前的复制偏移量，***slave\*** 等到自己的复制偏移量达到该值后，才会开始执行故障转移流程：发起选举、统计选票、赢得选举、升级为 ***master\*** 并更新配置

CLUSTER  FAILOVER 命令支持两个选项：***FORCE\*** 和 ***TAKEOVER\***。使用这两个选项，可以改变上述的流程。

如果有 ***FORCE\*** 选项，则 ***slave\*** 不会与 ***master\*** 进行交互，***master\*** 也不会阻塞其客户端，而是 ***slave\*** 立即开始故障转移流程：发起选举、统计选票、赢得选举、升级为 ***master\*** 并更新配置。

如果有 ***TAKEOVER\*** 选项，则更加简单直接，***slave\*** 不再发起选举，而是直接将自己升级为 ***master\*** ，接手原 ***master\*** 的 ***slot\***，增加自己的 ***configEpoch\*** 后更新配置。

因此，使用 ***FORCE\*** 和 ***TAKEOVER\*** 选项，***master\*** 可以已经下线；而不使用任何选项，只发送 CLUSTER  FAILOVER 命令的话，***master\*** 必须在线。

### 2.4.6. 详解 Redis 集群中的消息

搭建 Redis Cluster 时，首先通过 CLUSTER MEET 命令将所有的节点加入到一个集群中，但是并没有在所有节点两两之间都执行 CLUSTER MEET 命令，因为节点之间使用 ***Gossip\*** 协议进行工作。***Gossip\*** 翻译过来就是流言，类似与病毒传播一样，只要一个人感染，如果时间足够，那么和被感染的人在一起的所有人都会被感染，因此随着时间推移，集群内的所有节点都会互相知道对方的存在。

在 Redis 集群中，节点信息是如何传播的呢？答案是通过发送 PING 或 PONG 消息时，会包含节点信息，然后进行传播的。先介绍一下 Redis Cluster 中，消息是如何抽象的。一个消息对象可以是 ***PING、PONG、MEET\***，也可以是 ***PUBLISH、FAIL\*** 等。他们都是 clusterMsg 类型的结构，该类型主要由消息包头部和消息数据组成。

- 消息包头部包含签名、消息总大小、版本和发送消息节点的信息。
- 消息数据则是一个联合体 union clusterMsgData，联合体中又有不同的结构体来构建不同的消息。

***PING、PONG、MEET\*** 属于一类，是 clusterMsgDataGossip 类型的数组，可以存放多个节点的信息，该结构如下：

```plain
/* Initially we don't know our "name", but we'll find it once we connect
 * to the first node, using the getsockname() function. Then we'll use this
 * address for all the next messages. */
typedef struct {
    // 节点名字
    char nodename[CLUSTER_NAMELEN];
    // 最近一次发送PING的时间戳
    uint32_t ping_sent;
    // 最近一次接收PONG的时间戳
    uint32_t pong_received;
    // 节点的IP地址
    char ip[NET_IP_STR_LEN];  /* IP address last time it was seen */
    // 节点的端口号
    uint16_t port;              /* port last time it was seen */
    // 节点的标识
    uint16_t flags;             /* node->flags copy */
    // 未使用
    uint16_t notused1;          /* Some room for future improvements. */
    uint32_t notused2;
} clusterMsgDataGossip;
```

每次发送 ***MEET、PING、PONG\*** 消息时，发送者都从自己的已知节点列表中随机选出两个节点（可以是主节点或者从节点），并将这两个被选中节点的信息分别保存到两个结构中。当接收者收到消息时，接收者会访问消息正文中的两个结构，并根据自己是否认识 clusterMsgDataGossip 结构中记录的被选中节点进行操作：

1. 如果被选中节点不存在于接收者的已知节点列表，那么说明接收者是第一次接触到被选中节点，接收者将根据结构中记录的IP地址和端口号等信息，与被选择节点进行握手。
2. 如果被选中节点已经存在于接收者的已知节点列表，那么说明接收者之前已经与被选中节点进行过接触，接收者将根据 clusterMsgDataGossip 结构记录的信息，对被选中节点对应的 clusterNode 结构进行更新。

有了消息之后，如何选择发送消息的目标节点呢？虽然 ***PING PONG\*** 发送的频率越高就可以越实时得到其它节点的状态数据，但 ***Gossip\*** 消息体积较大，高频发送接收会加重网络带宽和消耗 CPU 的计算能力，因此每次 Redis 集群都会有目的性地选择一些节点；但节点选择过少又会影响故障判断的速度，Redis 集群的 ***Gossip\*** 协议选择这样的解决方案：

### 2.4.7. 集群数据一致性

Redis 集群尽可能保证数据的一致性，但在特定条件下会丢失数据，原因有两点：异步复制机制以及可能出现的网络分区造成脑裂问题。

#### 2.4.7.1.  异步复制

***master\*** 以及对应的 ***slaves\*** 之间使用异步复制机制，考虑如下场景：

写命令提交到 ***master\***，***master\*** 执行完毕后向客户端返回 ***OK\***，但由于复制的延迟此时数据还没传播给 ***slave\***；如果此时 ***master\*** 不可达的时间超过阀值，此时集群将触发 ***failover\***，将对应的 ***slave\*** 选举为新的***master\***，此时由于该 ***slave\*** 没有收到复制流，因此没有同步到 ***slave\*** 的数据将丢失。

##### 2.4.7.1.1.  **脑裂(split-brain)**

在发生网络分区时，有可能出现新旧 ***master\*** 同时存在的情况，考虑如下场景：

由于网络分区，此时 ***master\*** 不可达，且客户端与 ***master\*** 处于一个分区，并且由于网络不可达，此时客户端仍会向 ***master\*** 写入。由于 ***failover\*** 机制，将其中一个 ***slave\*** 提升为新的 ***master\***，等待网络分区消除后，老的 ***master\*** 再次可达，但此时该节点会被降为 ***slave\*** 清空自身数据然后复制新的 ***master\*** ，而在这段网络分区期间，客户端仍然将写命令提交到老的 ***master\***，但由于被降为 ***slave\*** 角色这些数据将永远丢失。

## 2.5. 淘汰策略

当Redis的内存空间已经用满时，Redis将根据配置的淘汰策略（maxmemory-policy），进行相应的动作。

Redis的淘汰策略共分为以下六种：

- no-eviction：不删除策略

当达到最大内存限制时，如果还需要更多的内存：直接返回错误

- allkeys-lru

当达到最大内存限制时，如果还需要更多的内存：在所有的key中，挑选最近最少使用（LRU）的key淘汰

- volatile-lru

当达到最大内存限制时，如果还需要更多的内存：在设置了expire（过期时间）的key中，挑选最近最少使用（LRU）的key淘汰

- allkeys-random

当达到最大内存限制时，如果还需要更多的内存：在所有的key中，随机淘汰部分key

- volatile-random

当达到最大内存限制时，如果还需要更多的内存：在设置了expire（过期时间）的key中，随机淘汰部分key

- volatile-ttl

当达到最大内存限制时，如果还需要更多的内存：在设置了expire（过期时间）的key中，挑选TTL（time to live，剩余时间）短的key淘汰

## 2.6. 过期策略

Redis可以为键值设置生存周期（TTL），并在过期之后自动删除这些键值对。

### 2.6.1. 过期键的相关命令

Redis 提供了 ***EXPIRE（PEXPIRE)\*** 和 ***EXPIREAT（PEXPIREAT）\***两个命令以秒或者毫秒精度来设置过期时间，区别是前者是生存时间，后者是具体的过期时间戳。

只有当键值被删除或者值被覆盖的时候，例如执行[DEL](https://redis.io/commands/del), [SET](https://redis.io/commands/set), [GETSET](https://redis.io/commands/getset) 和所有*STORE相关的命令，过期时间会被移除；而修改值的操作如[INCR](https://redis.io/commands/incr), [LPUSH](https://redis.io/commands/lpush), [HSET](https://redis.io/commands/hset)不会影响过期时间。另外，我们也可以通过***PERSIST\***命令手动移除键的过期时间

***TTL\***和***PTTL\***两个命令可以以秒或者毫秒精度查询键的剩余生存时间

### 2.6.2.  过期键的删除策略

当键过期时，有三种策略可以用来确定删除的时间：

- 定时删除：在设置键的过期时间的同时，创建一个定时器，让定时器在键的过期时间来临时，立即执行对键的删除
- 惰性删除：放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。
- 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。

第一种和第三种是主动删除策略，第二种则为被动删除策略。目前redis采用的是惰性删除和定期删除两种策略的配合使用，主要考虑到CPU和内存之间的平衡。

| 删除策略 | 优点                    | 缺点                         |
| -------- | ----------------------- | ---------------------------- |
| 定时删除 | 内存友好                | CPU不友好                    |
| 惰性删除 | CPU优化                 | 内存不友好                   |
| 定期删除 | 减少了对CPU和内存的影响 | 难以确定操作执行的时长和频率 |

### 2.6.3. AOF、RDB和复制功能对过期键的处理

#### 2.6.3.1. AOF

- AOF文件写入时，某个键已经过期，但还没有被惰性删除或者定期删除，那么AOF文件不会因为这个过期键产生任何影响。只有当键被删除后，AOF会追加一条DEL命令。
- AOF重写时，程序会对键进行检查，已经过期的键不会保存到重写的AOF文件中。

#### 2.6.3.2. RDB

- 生成RDB文件时，程序会对数据库中的键进行检查，已过期的键不会保存到新创建的RDB文件中。
- 载入RDB文件时，主节点和从节点采取不同的策略：
- 主节点会对文件中保存对键进行检查，未过期的键会被载入到数据库，过期的键则会被忽略。
- 从节点会把文件中包含的所有键，无论过期与否，都载入到数据库中。

#### 2.6.3.3. 复制

复制模式下，过期键的删除动作由主节点控制：

- 主节点在删除一个过期键之后，会显式地向所有从节点发送一个DEL命令
- 从节点在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，只有在接受到主节点发送的DEL命令之后，才会删除过期键。

## 2.7. 容灾机制

### 2.7.1. 术语定义

- redis集群少数派：发生网络分区时，包含主节点较少的分区，此分区如果不对从节点强制failover，将不能提供服务
- redis集群多数派：发生网络分区时，包含主节点较多的分区，此分区从节点会自动failover，可以正常提供读写服务
- zk集群少数派：发生网络分区时，包含节点较少分区，此分区不能提供服务
- zk集群多数派：发生网络分区时，包含节点较多分区，此分区可以正常提供读写服务

### 2.7.2. 设计目标

- 保证断网或者断电情况下HA自身的高可用
- 在发生网络分区时候，保证redis集群多数派的可用性，及时刷新客户端路由，摘掉不可用节点
- 在发生网络分区时候，保证redis集群少数派的可用性(需要确定是强制failover还是给出列表让人工处理)

# 3. 详细设计

### 3.1.1. Squirrel HA高可用

Squirrel HA作为监控redis集群的系统，本身必须多机房部署，保证自身的高可用，HA的高可用主要包含以下两个问题：

- 目前是一个redis集群同时被一个HA监控，因为一个redis集群有多个不同机房的节点，所以必须确保HA自身网络没有问题
- 由于网络分区造成HA自身不可用或者HA自身处于redis集群少数派，此时无法准确监控redis集群多数派，进而无法通过修改多数派所在的zk来刷新客户端路由，此时需要客户端根据异常报错来主动刷新本地路由表

之前对于第一个问题，目前HA会从线上每个机房随机选择若干redis宿主机，发送ping命令，如果有其他两个机房都存在10台以上的宿主机ping不通，则认为HA自身网络存在问题。其实HA接入了zk，引入了leader争抢调度机制后，此步骤可以省略。对于第二个问题。可以依赖zk对HA进行选主，引入争抢式调度，保证一个集群同时有且仅有一个leader HA来监控。即使发生网络分区，也能通过zk来选举出一个新的leader HA

### 3.1.2. 网络分区

网络分区考虑以下两种情况：

- 三机房主从节点均匀部署的集群
- 机房主节点多数部署在同机房的集群

#### 3.1.2.1. 机房主从节点均匀部署的集群

左图是一个三机房主从均匀分布的3主3从的集群。发生网络分区前，gh-HA抢到了leader，dx-HA 处于watch阻塞状态。

假设现在gh断网，造成网络分区，右图是网络分区后的集群。可以看到由于gh断网，此时网络分成了2个区。dx，yf一个分区，gh单独一个分区。

另外因为网络分区，gh的zk集群处于zk少数派，无法提供任何读写服务，gh-HA失去leader。gh redis节点只剩下一主一从显然也无法提供服务。

此时dx的一个slave自动failover成为主节点，现在新集群由dx的两个主节点和yf的一主一从组成，继续提供服务。

dx和yf的zk集群也能继续提供服务，此时dx-HA抢到leader，开始监控新的redis集群。dx-HA会监控到集群拓扑结构变更，会刷新redis集群配置到zk集群。这样处于dx和yf的业务客户端监听到zk发生变化，主动刷新本地路由，会从本地路由表里面摘除gh的redis节点。

#### 3.1.2.2. 机房多数主节点集中在一个机房的集群(断网的是主节点多的机房)

左图是网络分区前：redis集群由3主6从组成，其中3个主节点都在gh机房，此时gh-HA是leader，监控着redis集群右图gh断网是网络分区后：此时gh的三个主节点组建成一个新集群，继续提供服务。此时因为gh-zk处于少数派，已经无法提供任何读写服务。所以此时已经服务通过gh-HA刷新zk通知客户端更新本地路由(摘掉dx和yf节点)，这种情况需要客户端自身通过异常报错来及时刷新本地路由。

此时对dx和yf组成的分区，有两种操作:

- 上面已经提到过了，可以对dx和yf的redis从节点强制failover，这样dx和yf的节点可以组成一个新的redis集群提供服务。并且此时dx-HA抢到了leader，可以刷新zk通知业务客户端刷新本地路由。此种情况缺点是如果网络恢复后，gh分区的三个节点会以从节点身份加入dx和yf的集群。这样gh的数据就丢失了，导致数据不一致。这种情况保证了分布式网络的AP，不保证C。

- 也可以不对dx和yf的从节点强制failover，让dba介入人工判断。这样这段时间dx和yf的redis节点就无法提供服务，此种情况网络恢复后，dx和yf节点会加入gh节点，数据还是一致的。这种情况保证了分布式网络的CP，不保证A。

#### 3.1.2.3. 机房多数主节点集中在一个机房的集群(断网的是主节点少的机房)

此种情况和上面类似，假设上面断网的是dx机房，redis多数派集群由gh和yf节点组成继续提供服务。此时gh-HA还是leader

可以刷新zk，进而通知业务客户端刷新本地路由表。

对于dx分区，也可以像上面一样强制failover或者由人工判断。

### 3.1.3. 结论

- 运维方面，保证集群三机房主节点均匀部署。对于非三机房部署的集群，保证单机房出现故障时候的可用性。
- 运维调度服务，每天检查集群是否满足三机房部署，发出通知人工治理或者自动治理。
- 对于三机房均匀部署的集群，理论上单机房发生网络隔离，保证其他机房redis集群的可用性。网络隔离机房

网络恢复期间无法提供正常服务。网络恢复后，重新加入集群。