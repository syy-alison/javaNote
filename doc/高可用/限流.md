# 1. 为什么要限流

1. 如果对接N多个上游服务（恶意攻击，秒杀，恶意爬虫等），入口流量很难得到控制，存在被拖垮的风险。
2. 如果集群中一台主机突然出现CPU飙升
3. 流量不均导致单台机器请求量突然增大

# 2. 限流策略

| 类型           | 策略名称                                                     | 策略解释                                                     |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 全局策略       | 黑白名单策略                                                 | 请求资源命中白名单，直接通过；请求资源命中黑名单，直接拒绝； |
| 百分百策略     | 按一定比例拒绝请求，同时可以配置白名单，不参与百分比计算。   |                                                              |
| 自定义策略     | 集群限频                                                     | 适用场景为限制每个用户的访问次数、限制每个服务（AppKey）的调用次数。通过将用户ID（或者服务的AppKey）传给限流器的UUID参数来实现。1.0策略基于Redis计数实现，2.0策略基于自建的限流服务器计数实现，由于是统一计数，会存在通信消耗。 |
| 集群配额       | 一般与集群限频配合使用。面向场景：除了限制用户每秒的访问次数外，还需要限制用户每天访问总数的最大值。 |                                                              |
| 单机限流       | 基于Google的Guava令牌桶算法实现，默认缓存1秒钟的令牌用来应对突发流量。 |                                                              |
| 集群精确限流   | 单机限流通常情况下能满足大部分的限流需求，不过有些业务需求需要精确的限流，比如限制服务AppKey调用的次数。实现上与集群限频/集群配额一样。跟“集群限频/配额”的差异是：限频策略必须有参数，且参数的key必须是UUID；限流策略可以没有参数，如果有参数，参数的key只需要后台配置跟代码中的key保持一致即可。限频策略对UUID参数的每一个值都是独立的一个限流器；限流策略只有一个限流器，如果配置了参数，则只有命中了参数才会被这个限流器计数、限流。 |                                                              |
| 集群非精确限流 | 基于单机限流实现，根据服务节点数（OCTO上注册）平分集群总阈值到每台机器上，适用服务下每台机器负载均衡的场景。Rhino会监听OCTO上服务节点状态变化，单机QPS会随着可用节点数量的变化动态调整。 |                                                              |
| 兜底策略       | 自适应限流                                                   | 单机总限流策略，可以作为服务兜底。以CPU使用率为指标，通过PID控制算法，通过限流维持CPU利用率在阈值附近 |
| 单机总限流     | 单机总限流策略，配置一个单机的总阈值，所有的资源参与限流计数 |                                                              |

# 3. 算法

## 3.1. 令牌桶算法

![img](https://cdn.nlark.com/yuque/0/2024/png/42819892/1715775476364-221812a6-0c11-4241-b35d-e58af9718280.png)

**Guava限流器是基于令牌桶算法实现的。令牌桶算法是一种针对请求速度的限流算法，算法的核心就是一个令牌桶，桶内令牌数量以固定的速度增长。**

每个请求都需要从桶中获取到足够多的令牌后才能被放行，否则就会被阻塞或者拒绝。

通过这样一个过程，用于只需要设定令牌生成速度，算法就可以通过令牌发放来限制请求通过的速度。

**优点**：

- 流量整形和方便处理突发流量。
- 可以限制平均速率和应对突然激增的流量。
- 可以动态调整生成令牌的速率。

**不足：**

- 并发请求，存在未达到设置阈值也会被限流的情况;
- 如果令牌产生速率和桶的容量设置不合理，可能会出现问题比如大量的请求被丢弃、系统过载。
- 相比于其他限流算法，实现和理解起来更复杂一些。

流量整形是指令牌桶算法通过阻塞、拒绝等手段使请求以稳定的速度通过限流器，原本不规则的流量在经过限流器后变得平滑且均匀。流量整形效果非常有利于服务端稳定运行，类似我们在高并发系统中常用的基于消息队列实现的“削峰填谷”手段，经过整形后，服务端能够以稳定的状态接收并处理请求。

突发流量是指随机出现的、短时间的流量突刺。如果严格遵循流量整形的限制，那么服务端在遇到突发流量时会突然拒绝一大波请求，在客户端有重试机制的情况下还可能导致情况进一步恶化。因此，在服务端资源充足的条件下，限流器应该具有一些“弹性”，允许服务端临时超频处理一些突发请求。

在令牌桶算法模型中，“弹性”处理突发流量是非常容易实现的，只需要给桶中生成的令牌设置一个有效期即可。有突发流量时，限流器可以使用有效期内的剩余令牌来通过更多请求，从而临时提高服务端处理效率，避免大量请求被拒绝。

**采用令牌桶算法策略：单机限流、单机总限流、集群非精确限流**

常见问题：QPS设置100，那么每10ms会产生一个令牌，在出现并发情况，即使1s未达到100个请求，也存在被限流的情况

## 3.2. 固定窗口计数

![img](https://cdn.nlark.com/yuque/0/2024/png/42819892/1715775936927-d1662f61-6194-4f4c-b01c-648744087fa4.png)

基于Redis INCRBY + 过期时间来实现

![img](https://cdn.nlark.com/yuque/0/2024/png/42819892/1715776295376-8048a92e-58b5-454c-aee3-c40515cf2525.png)

**原理**：在一定时间内，对处理的请求数进行计数，每次到达时间临界点则计数器清零。在一定时间间隔内，若计数器数值超限，则进行限流。

**优点**：实现简单，单个计数周期不会超过配置阈值

**不足**：在两端临界点可能出现两倍的流速，流量不均匀

**应用：**集群精确限流1.0、集群限频1.0、集中式限流

## 3.3. 滑动窗口

![img](https://cdn.nlark.com/yuque/0/2024/png/42819892/1715776976808-346825ab-8d2d-4de2-8c39-a5ec5177a62c.png)

滑动窗口是是一种常用的计数算法，如上图所示，窗口大小是1s，1个窗口内分配了5个桶，每个桶的时间间隔为200ms。具体的流程：

1.首先获取到当前的时间戳，用当前的时间戳除以桶的时间间隔，获取到当前桶的索引数，一般情况下桶的数量都是有限的，会前后相连形成一个环，获取到桶的索引一般还是与桶的数量取余操作；

2.获取到具体桶的位置，需要判断当前桶的数据是否过期，所以需要计算当前桶的起始时间，然后与当前桶的起始时间对比，如果不一致，则当前桶过期，数据清空，重新计数；

3.具体到限流的判断，则是获取到最近几个连续桶的数据相加与限流阈值对比，小于则通过，否则直接拒绝。

**采用滑动窗口算法使用策略：集群精确限流2.0、集群限频2.0**

## 3.4. 漏桶算法

![img](https://cdn.nlark.com/yuque/0/2024/png/42819892/1715779096677-aa91c5c1-eab1-42a4-a0c6-d15b58a84bf3.png)

我们可以把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为漏桶漏水。我们往桶中以任意速率流入水，以一定速率流出水。当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。

如果想要实现这个算法的话也很简单，准备一个队列用来保存请求，然后我们定期从队列中拿请求来执行就好了（和消息队列削峰/限流的思想是一样的）。

优点：

- 实现简单，易于理解。
- 可以控制限流速率，避免网络拥塞和系统过载。

缺点：

- 无法应对突然激增的流量，因为只能以固定的速率处理请求，对系统资源利用不够友好。
- 桶流入水（发请求）的速率如果一直大于桶流出水（处理请求）的速率的话，那么桶会一直是满的，一部分新的请求会被丢弃，导致服务质量下降。

实际业务场景中，基本不会使用漏桶算法。

## 3.5. 自适应限流

根据系统的负载情况（CPU使用率），自动决策请求是否通过，降级了用户的使用成本

![img](https://cdn.nlark.com/yuque/0/2024/png/42819892/1715777245626-308266fc-df2c-4b33-8986-f60b9769a304.png)



# 4. 单机限流

单机限流针对的是单体架构应用。

单机限流可以直接使用 Google Guava 自带的限流工具类 RateLimiter 。 RateLimiter 基于令牌桶算法，可以应对突发流量。

除了最基本的令牌桶算法(平滑突发限流)实现之外，Guava 的RateLimiter还提供了 **平滑预热限流** 的算法实现。

平滑突发限流就是按照指定的速率放令牌到桶里，而平滑预热限流会有一段预热时间，预热时间之内，速率会逐渐提升到配置的速率。

## 4.1 Guava 限流



# 5. 分布式限流

分布式限流针对的分布式/微服务应用架构应用，在这种架构下，单机限流就不适用了，因为会存在多种服务，并且一种服务也可能会被部署多份。

分布式限流常见的方案：

- **借助中间件限流**：可以借助 Sentinel 或者使用 Redis 来自己实现对应的限流逻辑。
- **网关层限流**：比较常用的一种方案，直接在网关层把限流给安排上了。不过，通常网关层限流通常也需要借助到中间件/框架。就比如 Spring Cloud Gateway 的分布式限流实现RedisRateLimiter就是基于 Redis+Lua 来实现的，再比如 Spring Cloud Gateway 还可以整合 Sentinel 来做限流。

